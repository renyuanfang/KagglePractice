{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os, re, csv, codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding\n",
    "from keras.layers import Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "import gc\n",
    "import gensim.models.keyedvectors as word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('jigsaw/train.csv')\n",
    "test = pd.read_csv('jigsaw/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_classes = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "y = train[list_classes].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sentences_train = train['comment_text']\n",
    "list_sentences_test = test['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer\n",
    "max_num_words = 20000\n",
    "tokenizer = Tokenizer(num_words = max_num_words)\n",
    "tokenizer.fit_on_texts(list_sentences_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 200\n",
    "x_train = pad_sequences(list_tokenized_train, maxlen = maxlen)\n",
    "x_test = pad_sequences(list_tokenized_test, maxlen = maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadEmbeddingMatrix(typeToLoad):\n",
    "    if typeToLoad == 'glove':\n",
    "        embedding_file = 'wordEmbedding/glove.twitter.27B.25d.txt'\n",
    "        embed_size = 25\n",
    "    elif typeToLoad == 'word2vec':\n",
    "        embedding_file = 'wordEmbedding/GoogleNews-vectors-negative300.bin'\n",
    "        word2vecDict = word2vec.KeyedVectors.load_word2vec_format(embedding_file, binary = True)\n",
    "        embed_size = 300\n",
    "    elif typeToLoad == 'fasttext':\n",
    "        embedding_file = 'wordEmbedding/wiki-news-300d-1M.vec'\n",
    "        embed_size = 300\n",
    "    \n",
    "    if typeToLoad == 'glove':\n",
    "        embedding_index = dict()\n",
    "        f = open(embedding_file)\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype = 'float32')\n",
    "            embedding_index[word] = coefs\n",
    "        f.close()\n",
    "    elif typeToLoad == 'fasttext':\n",
    "        embedding_index = dict()\n",
    "        f = open(embedding_file, 'r', encoding = 'utf-8', newline = '\\n', errors = 'ignore')\n",
    "        f.readline() #skip first line\n",
    "        for line in f:\n",
    "            values = line.rstrip().split(' ')\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype = 'float32')\n",
    "            embedding_index[word] = coefs\n",
    "        f.close()\n",
    "    elif typeToLoad == 'word2vec':\n",
    "        embedding_index = dict()\n",
    "        for word in word2vecDict.wv.vocab:\n",
    "            embedding_index[word] = word2vecDict.word_vec(word)\n",
    "    \n",
    "    nb_words = min(len(tokenizer.word_index), max_num_words) + 1\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    gc.collect()\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        if i > max_num_words:\n",
    "            continue\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    del embedding_index\n",
    "    gc.collect()\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = loadEmbeddingMatrix('fasttext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words, embed_size = embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 200, 300)          6000300   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200, 120)          173280    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                6050      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 6,179,936\n",
      "Trainable params: 179,636\n",
      "Non-trainable params: 6,000,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape = (maxlen,))\n",
    "x = Embedding(num_words, embed_size, embeddings_initializer=Constant(embedding_matrix), trainable = False)(inp)\n",
    "x = Bidirectional(LSTM(60, return_sequences = True, name = 'lstm_layer', dropout = 0.1, recurrent_dropout = 0.1))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(50, activation='relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation = 'sigmoid')(x)\n",
    "    \n",
    "model = Model(inp, x)\n",
    "model.compile(loss = 'binary_crossentropy',\n",
    "                 optimizer = 'adam',\n",
    "                 metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/4\n",
      "143613/143613 [==============================] - 1280s 9ms/step - loss: 0.0482 - acc: 0.9818 - val_loss: 0.0451 - val_acc: 0.9830\n",
      "Epoch 2/4\n",
      "143613/143613 [==============================] - 1251s 9ms/step - loss: 0.0443 - acc: 0.9830 - val_loss: 0.0456 - val_acc: 0.9825\n",
      "Epoch 3/4\n",
      "143613/143613 [==============================] - 1276s 9ms/step - loss: 0.0423 - acc: 0.9836 - val_loss: 0.0447 - val_acc: 0.9828\n",
      "Epoch 4/4\n",
      "143613/143613 [==============================] - 1286s 9ms/step - loss: 0.0406 - acc: 0.9841 - val_loss: 0.0440 - val_acc: 0.9834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fad5d0e22e8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 4\n",
    "model.fit(x_train, y, batch_size=batch_size, epochs = epochs, validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test = x_test.shape[0]\n",
    "prediction = np.zeros((num_test, 6))\n",
    "i = 0\n",
    "while True:\n",
    "    bounded = min(num_test, i + batch_size)\n",
    "    p = model.predict(x_test[i:bounded])\n",
    "    prediction[i:bounded] = p\n",
    "    i = bounded\n",
    "    if bounded == num_test:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = prediction > 0.5\n",
    "res = np.asarray(a, dtype = np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "docid = np.asarray(test['id']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = pd.DataFrame({'id':docid, 'toxic':prediction[:,0], 'severe_toxic':prediction[:,1], 'obscene':prediction[:,2], 'threat':prediction[:,3], 'insult':prediction[:,4], 'identity_hate':prediction[:,5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre.to_csv(\"submission.csv\",index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
